{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment3.ipynb","provenance":[{"file_id":"1PhNPpklp9FbxJEtsZ8Jp9qXQa4aZDK5Y","timestamp":1578382811452}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"c9QcGnGPdX2C","colab_type":"text"},"source":["\n","**Install requirements**"]},{"cell_type":"code","metadata":{"id":"k9O3aM3Tb28q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"outputId":"7307392b-f031-4764-9e70-2b50bc04573b","executionInfo":{"status":"ok","timestamp":1578566199595,"user_tz":-60,"elapsed":16429,"user":{"displayName":"Matteo Borghesi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAJ006jjLoZQkDO_4jgsI0EePYxnVz0NHkD1gP1Ig=s64","userId":"05356648325831651391"}}},"source":["!pip3 install 'torch==1.3.1'\n","!pip3 install 'torchvision==0.4.2'\n","!pip3 install 'Pillow-SIMD'\n","!pip3 install 'tqdm'"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.17.5)\n","Requirement already satisfied: torchvision==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.12.0)\n","Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.17.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (6.2.2)\n","Requirement already satisfied: Pillow-SIMD in /usr/local/lib/python3.6/dist-packages (6.0.0.post0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fo942LMOdlh4","colab_type":"text"},"source":["**Import libraries**"]},{"cell_type":"code","metadata":{"id":"DokFOdD1dJEl","colab_type":"code","colab":{}},"source":["import os\n","import logging\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Subset, DataLoader\n","from torch.backends import cudnn\n","\n","import torchvision\n","from torchvision import transforms\n","from torchvision.models import alexnet\n","\n","from PIL import Image\n","from tqdm import tqdm\n","\n","import torch.hub\n","from torch.hub import load_state_dict_from_url\n","from torch.autograd import Function\n","\n","import copy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OIDLJuIXK_vh","colab_type":"text"},"source":["**Set Arguments**"]},{"cell_type":"code","metadata":{"id":"d5PkYfqfK_SA","colab_type":"code","colab":{}},"source":["DEVICE = 'cuda' # 'cuda' or 'cpu'\n","\n","NUM_CLASSES = 7 # 101 + 1: There is am extra Background class that should be removed \n","\n","BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n","                     # the batch size, learning rate should change by the same factor to have comparable results\n","\n","LR = 1e-3            # The initial Learning Rate\n","MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n","\n","GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n","\n","LOG_FREQUENCY = 5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9gwii0TBHvzh","colab_type":"text"},"source":["**Define Data Preprocessing**"]},{"cell_type":"code","metadata":{"id":"QUDdw4j2H0Mc","colab_type":"code","colab":{}},"source":["# Define transforms for training phase\n","transf = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n","                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n","                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n","                                                                   # Remember this when applying different transformations, otherwise you get an error\n","                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n","                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # Normalizes tensor with mean and standard deviation\n","])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2qYIHPzYLY7i","colab_type":"text"},"source":["**Prepare Dataset**"]},{"cell_type":"code","metadata":{"id":"QfVq_uDHLbsR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"5677b2f5-e450-4400-9fd6-fa8675004f0b","executionInfo":{"status":"ok","timestamp":1578566200447,"user_tz":-60,"elapsed":17240,"user":{"displayName":"Matteo Borghesi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAJ006jjLoZQkDO_4jgsI0EePYxnVz0NHkD1gP1Ig=s64","userId":"05356648325831651391"}}},"source":["# Clone github repository with data\n","if not os.path.isdir('./Homework3-PACS'):\n","  !git clone https://github.com/MachineLearning2020/Homework3-PACS\n","\n","PHOTO_DIR = 'Homework3-PACS/PACS/photo'\n","ART_DIR = 'Homework3-PACS/PACS/art_painting'\n","CARTOON_DIR = 'Homework3-PACS/PACS/cartoon'\n","SKETCH_DIR = 'Homework3-PACS/PACS/sketch'\n","\n","# Prepare Pytorch train/test Datasets\n","photo_dataset = torchvision.datasets.ImageFolder(PHOTO_DIR, transform=transf)\n","art_dataset = torchvision.datasets.ImageFolder(ART_DIR, transform=transf)\n","cartoon_dataset = torchvision.datasets.ImageFolder(CARTOON_DIR, transform=transf)\n","sketch_dataset = torchvision.datasets.ImageFolder(SKETCH_DIR, transform=transf)\n","\n","# Check dataset sizes\n","print('Photo Dataset: {}'.format(len(photo_dataset)))\n","print('Art Dataset: {}'.format(len(art_dataset)))\n","print('Cartoon Dataset: {}'.format(len(cartoon_dataset)))\n","print('Sketch Dataset: {}'.format(len(sketch_dataset)))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Photo Dataset: 1670\n","Art Dataset: 2048\n","Cartoon Dataset: 2344\n","Sketch Dataset: 3929\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FYEDQ7Z21ldN","colab_type":"text"},"source":["**Prepare Dataloaders**"]},{"cell_type":"code","metadata":{"id":"VriRw8SI1nle","colab_type":"code","colab":{}},"source":["# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n","photo_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","art_dataloader = DataLoader(art_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","cartoon_dataloader = DataLoader(cartoon_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","sketch_dataloader = DataLoader(sketch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gbZ1t5Qs2z4j","colab_type":"text"},"source":["**Prepare Network**"]},{"cell_type":"code","metadata":{"id":"exHUjtXa22DN","colab_type":"code","colab":{}},"source":["class ReverseLayerF(Function):\n","    # Forwards identity\n","    # Sends backward reversed gradients\n","    @staticmethod\n","    def forward(ctx, x, alpha):\n","        ctx.alpha = alpha\n","\n","        return x.view_as(x)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        output = grad_output.neg() * ctx.alpha\n","\n","        return output, None\n","\n","class DANN(nn.Module):\n","\n","    def __init__(self, num_classes=1000):\n","        super(DANN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, 1000),\n","        )\n","        self.GD = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, 2),\n","        )\n","\n","\n","    def forward(self, x, alpha=None):\n","        features = self.features(x)\n","        # Flatten the features:\n","        features = features.view(features.size(0), -1)\n","        # If we pass alpha, we can assume we are training the discriminator\n","        if alpha is not None:\n","            # gradient reversal layer (backward gradients will be reversed)\n","            reverse_feature = ReverseLayerF.apply(features, alpha)\n","            discriminator_output = self.GD(reverse_feature)\n","            return discriminator_output\n","        # If we don't pass alpha, we assume we are training with supervision\n","        else:\n","            class_outputs = self.classifier(features)\n","            return class_outputs\n","\n","\n","def dann(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"AlexNet model architecture from the\n","    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    model = DANN(**kwargs)\n","    if pretrained:\n","        state_dict = load_state_dict_from_url('https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n","                                              progress=progress)        \n","        model.load_state_dict(state_dict, strict=False)\n","\n","        #model.GD[1] = model.classifier[1].state_dict())\n","        #model.GD[4].load_state_dict(model.classifier[4].state_dict())\n","        \n","        model.classifier[6] = nn.Linear(4096, 7)\n","        model.GD[1].weight.data = copy.deepcopy(model.classifier[1].weight.data)\n","        model.GD[1].bias.data = copy.deepcopy(model.classifier[1].bias.data)\n","        model.GD[4].weight.data = copy.deepcopy(model.classifier[4].weight.data)\n","        model.GD[4].bias.data = copy.deepcopy(model.classifier[4].bias.data)\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AxYUli9d9uYQ","colab_type":"text"},"source":["**Prepare training**"]},{"cell_type":"code","metadata":{"id":"w_0XpkDwdtd3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"d1f1daec-2d5d-4ac9-f586-ef2c4d9e4a7d","executionInfo":{"status":"ok","timestamp":1578581669389,"user_tz":-60,"elapsed":3148,"user":{"displayName":"Matteo Borghesi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAJ006jjLoZQkDO_4jgsI0EePYxnVz0NHkD1gP1Ig=s64","userId":"05356648325831651391"}}},"source":["cudnn.benchmark # Calling this optimizes runtime\n","\n","# Hyperparameters for grid search\n","NUM_EPOCHS = 15      # Total number of training epochs (iterations over dataset)\n","STEP_SIZE = 10      # How many epochs before decreasing learning rate (if using a step-down policy)\n","ALPHA = 0.8\n","\n","transfer_set = 'sketch' # can be 'cartoon' or 'sketch'\n","DA_ENABLED = True\n","\n","source_dataloader = photo_dataloader\n","target_dataloader = -1\n","test_dataloader = art_dataloader\n","\n","net = dann(pretrained=True).to(DEVICE) # Loading model            \n","criterion = nn.CrossEntropyLoss()\n","parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n","optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","if transfer_set == 'cartoon':\n","  target_dataloader = cartoon_dataloader\n","elif transfer_set == 'sketch':\n","  target_dataloader = sketch_dataloader\n","\n","nr_batches = min(len(source_dataloader), len(target_dataloader))\n","print(\"Number of batches: %d\" % nr_batches)\n","            "],"execution_count":64,"outputs":[{"output_type":"stream","text":["Number of batches: 6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"W6mTeEnDeAI_"},"source":["**Train**"]},{"cell_type":"code","metadata":{"id":"ZcoQ5fD49yT_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7bb60baf-8186-436c-bd29-113b65ea6b8d","executionInfo":{"status":"ok","timestamp":1578582524869,"user_tz":-60,"elapsed":849112,"user":{"displayName":"Matteo Borghesi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAJ006jjLoZQkDO_4jgsI0EePYxnVz0NHkD1gP1Ig=s64","userId":"05356648325831651391"}}},"source":["# Start iterating over the epochs\n","for epoch in range(NUM_EPOCHS):\n","  \n","  net.train(True)\n","\n","  print('\\nStarting epoch {}/{}, LR = {}  ALPHA = [{}] TARGET DATASET = [{}]'.format(epoch+1, \n","                                NUM_EPOCHS, scheduler.get_lr(), ALPHA, transfer_set))\n","          \n","  # Iterate over the dataset\n","  i = 0\n","  while i < nr_batches:          \n","    \n","    optimizer.zero_grad() # Zero-ing the gradients\n","\n","    # Load source batch\n","    source_images, source_labels = next(iter(source_dataloader))\n","    source_images = source_images.to(DEVICE)\n","    source_labels = source_labels.to(DEVICE)          \n","\n","    # Load target batch\n","    target_images, target_labels = next(iter(target_dataloader))\n","    target_images = target_images.to(DEVICE)          \n","\n","    # STEP 1: train the classifier\n","    outputs = net(source_images)          \n","    loss_class = criterion(outputs, source_labels)  \n","    if i % LOG_FREQUENCY == 0:\n","      print('Step {}, Loss Classifier {}'.format(i+1, loss_class.item()))                \n","    loss_class.backward()  # backward pass: computes gradients\n","\n","    if DA_ENABLED:\n","      # STEP 2: train the discriminator: forward SOURCE data to Gd          \n","      outputs = net.forward(source_images, alpha=ALPHA)           \n","      labels_discr_source = torch.zeros(BATCH_SIZE, dtype=torch.int64).to(DEVICE) # source's label is 0\n","      loss_discr_source = criterion(outputs, labels_discr_source)           \n","      if i % LOG_FREQUENCY == 0:\n","        print('Step {}, Loss Distriminator Source {}'.format(i+1, loss_discr_source.item()))\n","      loss_discr_source.backward()\n","\n","      # STEP 3: train the discriminator: forward TARGET to Gd          \n","      outputs = net.forward(target_images, alpha=ALPHA)           \n","      labels_discr_target = torch.ones(BATCH_SIZE, dtype=torch.int64).to(DEVICE) # target's label is 1\n","      loss_discr_target = criterion(outputs, labels_discr_target)       \n","      if i % LOG_FREQUENCY == 0:\n","        print('Step {}, Loss Discriminator Target {}'.format(i+1, loss_discr_target.item()))\n","      loss_discr_target.backward()    #update gradients \n","\n","    optimizer.step() # update weights based on accumulated gradients          \n","    i += 1\n","\n","  # Step the scheduler\n","  scheduler.step() \n","#print(\"Loss classifier\")\n","#print(loss_class_list)\n","#print(\"\\nLoss discriminator source\")\n","#print(loss_source_list)\n","#print(\"\\nLoss discriminator target\")\n","#print(loss_target_list)\n","\n","# now train is finished, evaluate the model on the target dataset \n","net.train(False) # Set Network to evaluation mode\n","  \n","running_corrects = 0\n","for images, labels in target_dataloader:\n","  images = images.to(DEVICE)\n","  labels = labels.to(DEVICE)\n","  \n","  outputs = net(images)\n","  _, preds = torch.max(outputs.data, 1)\n","  running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","# Calculate Accuracy\n","accuracy = running_corrects / float( len(target_dataloader)*(target_dataloader.batch_size) )\n","print(\"Accuracy: %f (%d / %d )\" % (accuracy, running_corrects, len(target_dataloader)*(target_dataloader.batch_size)))"],"execution_count":65,"outputs":[{"output_type":"stream","text":["\n","Starting epoch 1/15, LR = [0.001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier 2.2314772605895996\n","Step 1, Loss Distriminator Source 0.5516728162765503\n","Step 1, Loss Discriminator Target 1.2292377948760986\n","Step 6, Loss Classifier 0.8010777831077576\n","Step 6, Loss Distriminator Source 0.1531529277563095\n","Step 6, Loss Discriminator Target 8.23289155960083e-07\n","\n","Starting epoch 2/15, LR = [0.001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier 0.723768949508667\n","Step 1, Loss Distriminator Source 0.12396044284105301\n","Step 1, Loss Discriminator Target 7.264316082000732e-08\n","Step 6, Loss Classifier 0.308158814907074\n","Step 6, Loss Distriminator Source 0.23632672429084778\n","Step 6, Loss Discriminator Target 0.0\n","\n","Starting epoch 3/15, LR = [0.001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier 0.298658162355423\n","Step 1, Loss Distriminator Source 0.17480769753456116\n","Step 1, Loss Discriminator Target 1.6763806343078613e-08\n","Step 6, Loss Classifier 0.33884185552597046\n","Step 6, Loss Distriminator Source 1.5873680114746094\n","Step 6, Loss Discriminator Target 0.0\n","\n","Starting epoch 4/15, LR = [0.001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier 0.6893793344497681\n","Step 1, Loss Distriminator Source 4.744756698608398\n","Step 1, Loss Discriminator Target 0.0\n","Step 6, Loss Classifier 79681.6875\n","Step 6, Loss Distriminator Source 251629.421875\n","Step 6, Loss Discriminator Target 0.0\n","\n","Starting epoch 5/15, LR = [0.001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier 9.344867006572134e+16\n","Step 1, Loss Distriminator Source 0.0\n","Step 1, Loss Discriminator Target 2.666329991007961e+19\n","Step 6, Loss Classifier nan\n","Step 6, Loss Distriminator Source nan\n","Step 6, Loss Discriminator Target nan\n","\n","Starting epoch 6/15, LR = [0.001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier nan\n","Step 1, Loss Distriminator Source nan\n","Step 1, Loss Discriminator Target nan\n","Step 6, Loss Classifier nan\n","Step 6, Loss Distriminator Source nan\n","Step 6, Loss Discriminator Target nan\n","\n","Starting epoch 7/15, LR = [0.001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier nan\n","Step 1, Loss Distriminator Source nan\n","Step 1, Loss Discriminator Target nan\n","Step 6, Loss Classifier nan\n","Step 6, Loss Distriminator Source nan\n","Step 6, Loss Discriminator Target nan\n","\n","Starting epoch 8/15, LR = [0.001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier nan\n","Step 1, Loss Distriminator Source nan\n","Step 1, Loss Discriminator Target nan\n","Step 6, Loss Classifier nan\n","Step 6, Loss Distriminator Source nan\n","Step 6, Loss Discriminator Target nan\n","\n","Starting epoch 9/15, LR = [0.001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier nan\n","Step 1, Loss Distriminator Source nan\n","Step 1, Loss Discriminator Target nan\n","Step 6, Loss Classifier nan\n","Step 6, Loss Distriminator Source nan\n","Step 6, Loss Discriminator Target nan\n","\n","Starting epoch 10/15, LR = [0.001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier nan\n","Step 1, Loss Distriminator Source nan\n","Step 1, Loss Discriminator Target nan\n","Step 6, Loss Classifier nan\n","Step 6, Loss Distriminator Source nan\n","Step 6, Loss Discriminator Target nan\n","\n","Starting epoch 11/15, LR = [0.0001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier nan\n","Step 1, Loss Distriminator Source nan\n","Step 1, Loss Discriminator Target nan\n","Step 6, Loss Classifier nan\n","Step 6, Loss Distriminator Source nan\n","Step 6, Loss Discriminator Target nan\n","\n","Starting epoch 12/15, LR = [0.0001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier nan\n","Step 1, Loss Distriminator Source nan\n","Step 1, Loss Discriminator Target nan\n","Step 6, Loss Classifier nan\n","Step 6, Loss Distriminator Source nan\n","Step 6, Loss Discriminator Target nan\n","\n","Starting epoch 13/15, LR = [0.0001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier nan\n","Step 1, Loss Distriminator Source nan\n","Step 1, Loss Discriminator Target nan\n","Step 6, Loss Classifier nan\n","Step 6, Loss Distriminator Source nan\n","Step 6, Loss Discriminator Target nan\n","\n","Starting epoch 14/15, LR = [0.0001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier nan\n","Step 1, Loss Distriminator Source nan\n","Step 1, Loss Discriminator Target nan\n","Step 6, Loss Classifier nan\n","Step 6, Loss Distriminator Source nan\n","Step 6, Loss Discriminator Target nan\n","\n","Starting epoch 15/15, LR = [0.0001]  ALPHA = [0.8] TARGET DATASET = [sketch]\n","Step 1, Loss Classifier nan\n","Step 1, Loss Distriminator Source nan\n","Step 1, Loss Discriminator Target nan\n","Step 6, Loss Classifier nan\n","Step 6, Loss Distriminator Source nan\n","Step 6, Loss Discriminator Target nan\n","Accuracy: 0.195833 (752 / 3840 )\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UsHFI-GAJd69","colab_type":"text"},"source":["**Test**"]},{"cell_type":"code","metadata":{"id":"EO3HV5pqJg1o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"658348d6-8793-4ae3-cc9d-f46e994fd999","executionInfo":{"status":"ok","timestamp":1578585780591,"user_tz":-60,"elapsed":7655,"user":{"displayName":"Matteo Borghesi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAJ006jjLoZQkDO_4jgsI0EePYxnVz0NHkD1gP1Ig=s64","userId":"05356648325831651391"}}},"source":["net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n","net.train(False) # Set Network to evaluation mode\n","\n","running_corrects = 0\n","for images, labels in tqdm(test_dataloader):\n","  images = images.to(DEVICE)\n","  labels = labels.to(DEVICE)\n","\n","  # Forward Pass\n","  outputs = net(images)\n","\n","  # Get predictions\n","  _, preds = torch.max(outputs.data, 1)\n","\n","  # Update Corrects\n","  running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","# Calculate Accuracy\n","accuracy = running_corrects / float(len(art_dataset))\n","\n","print('\\nTest Accuracy: {} ({} / {})'.format(accuracy, running_corrects, len(art_dataset)))"],"execution_count":66,"outputs":[{"output_type":"stream","text":["100%|██████████| 8/8 [00:05<00:00,  1.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Test Accuracy: 0.18505859375 (379 / 2048)\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}